{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cdc9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718ee7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "# 1000 lines of benign training data\n",
    "# 100 lines of benign testing data, followed by 40 lines anomalies, followed by 60 lines of benign\n",
    "\n",
    "def gaussian_noise(data, mean=0.0, std=0.1):\n",
    "    noise = torch.randn_like(data) * std + mean\n",
    "    return data + noise\n",
    "\n",
    "# Now testing on anomaly data (salt-and-pepper noise)\n",
    "def salt_and_pepper_noise(data, prob=0.25):\n",
    "    noisy_data = data.clone()\n",
    "    rand = torch.rand_like(data)\n",
    "    noisy_data[rand < prob / 2] = 0.0\n",
    "    noisy_data[rand > 1 - prob / 2] = 1.0\n",
    "    return noisy_data\n",
    "\n",
    "data = gaussian_noise(torch.randn(1160, 16) * 0.1, mean=0.0, std=0.1)\n",
    "anomaly_data = salt_and_pepper_noise(torch.randn(100, 16), prob=0.25)\n",
    "# split into train and test\n",
    "train_data = data[:1000]\n",
    "test_data = torch.cat([data[1000:], anomaly_data], dim=0)\n",
    "test_labels = torch.cat([torch.zeros(160), torch.ones(100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65fab649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup autoencoder\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[64, 32, 8, 32]):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "            nn.LeakyReLU(True)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_sizes[3], hidden_sizes[2]),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[1]),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[0], input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd3dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0488\n",
      "Epoch [50/100], Loss: 0.0159\n",
      "Epoch [100/100], Loss: 0.0157\n"
     ]
    }
   ],
   "source": [
    "# Train autoencoder using ten-fold cross-validation\n",
    "\n",
    "autoencoder = Autoencoder(input_size=16, hidden_sizes=[12, 8, 4, 8])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "num_samples = train_data.size(0)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_data = train_data[i * batch_size:(i + 1) * batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(batch_data)\n",
    "        loss = criterion(outputs, batch_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_data.size(0)\n",
    "    epoch_loss /= num_samples\n",
    "    # if (epoch + 1) % 50 == 0:\n",
    "    if epoch == 0 or (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adede86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-level thresholds: [0.01970199 0.02574495 0.03248791 0.02874535 0.02742706 0.0244266\n",
      " 0.02699526 0.0315773  0.02706124 0.02571756 0.03147859 0.03194313\n",
      " 0.02230235 0.02254503 0.02363099 0.0281459 ]\n"
     ]
    }
   ],
   "source": [
    "# Compute reconstruction errors on training data\n",
    "\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    recon = autoencoder(train_data)\n",
    "    errors = (train_data - recon)**2               # shape: (N, 16)\n",
    "    sample_errors = errors.mean(dim=1)            # shape: (N,) â€“ per-sample MSE\n",
    "\n",
    "err_np = errors.cpu().numpy()                      # shape (N,16)\n",
    "\n",
    "feature_median = np.median(err_np, axis=0)         # shape (16,)\n",
    "feature_mad = np.median(np.abs(err_np - feature_median), axis=0)\n",
    "\n",
    "k = 3.0\n",
    "thresholds_feature = feature_median + k * feature_mad\n",
    "\n",
    "print(\"Feature-level thresholds:\", thresholds_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c50fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "# get reconstruction error matrix for data per feature\n",
    "\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(test_data)\n",
    "    reconstruction_errors = (test_data - reconstructed) ** 2  # MSE per feature\n",
    "    spikes = (reconstruction_errors.cpu().numpy() > thresholds_feature).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1eb984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average spikes per feature for normal data: [0.23125 0.2625  0.20625 0.19375 0.23125 0.175   0.21875 0.21875 0.2125\n",
      " 0.1375  0.1375  0.18125 0.25    0.24375 0.2625  0.2375 ]\n",
      "Average spikes per feature for anomaly data: [0.86 0.83 0.83 0.84 0.87 0.81 0.84 0.85 0.83 0.83 0.81 0.79 0.88 0.82\n",
      " 0.77 0.86]\n"
     ]
    }
   ],
   "source": [
    "# split spikes into normal and anomaly\n",
    "# count average number of spikes per sample for normal and anomaly\n",
    "normal_spikes = spikes[test_labels == 0]\n",
    "anomaly_spikes = spikes[test_labels == 1]\n",
    "avg_normal_spikes = normal_spikes.mean(axis=0)\n",
    "avg_anomaly_spikes = anomaly_spikes.mean(axis=0)\n",
    "print(\"Average spikes per feature for normal data:\", avg_normal_spikes)\n",
    "print(\"Average spikes per feature for anomaly data:\", avg_anomaly_spikes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c1e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuron(torch.nn.Module):\n",
    "    def __init__(self, tau=10.0, v_th=1.0):\n",
    "        super(LIFNeuron, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.v_th = v_th\n",
    "        self.v = torch.zeros(1)\n",
    " \n",
    "    def forward(self, input_spikes):\n",
    "        # Update membrane potential\n",
    "        self.v = (1 - 1/self.tau) * self.v + input_spikes\n",
    "        # Check if spike is fired\n",
    "        spikes = (self.v >= self.v_th).float()\n",
    "        # Reset membrane potential if spike is fired\n",
    "        self.v = (1 - spikes) * self.v\n",
    "        return spikes\n",
    "\n",
    "class SpikingNeuralNework(nn.Module):\n",
    "    def __init__(self, hidden_sizes=[16], input_size=16, output_size=2, tau=10.0, v_th=1.0):\n",
    "        super(SpikingNeuralNework, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.lif1 = LIFNeuron(tau=tau, v_th=v_th)\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], output_size)\n",
    "        self.lif2 = LIFNeuron(tau=tau, v_th=v_th)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lif1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lif2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abf384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer spikes for normal data: [18 43]\n",
      "Output layer spikes for anomaly data: [17 12]\n"
     ]
    }
   ],
   "source": [
    "snn = SpikingNeuralNework(hidden_sizes=[16], input_size=16, output_size=2, tau=10.0, v_th=0.2)\n",
    "output = snn(torch.FloatTensor(spikes))\n",
    "# count spikes and non-spikes, per normal and anomaly\n",
    "output_spikes = (output.detach().numpy() > 0).astype(int)\n",
    "normal_output_spikes = output_spikes[test_labels.numpy() == 0]\n",
    "anomaly_output_spikes = output_spikes[test_labels.numpy() == 1]\n",
    "print(\"Output layer spikes for normal data:\", normal_output_spikes.sum(axis=0))\n",
    "print(\"Output layer spikes for anomaly data:\", anomaly_output_spikes.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642730e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a simple test of the spiking neural network\n",
    "# Create a 2 input dataset and the expected output.\n",
    "# inputs: [0,0], [0,1], [1,0], [1,1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
